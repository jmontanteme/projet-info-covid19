{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Initialisation de la base de donnée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook a pour objectif de prendre la base de donnée et les metadatas, de les traiter pour noter les articles. Le but est de sauvegarder deux csv, un pour ceux avec texte complet et l'autre non. Ils contiennent des notes pour la date ainsi que les titres et abstracts transformés. Celui avec les textes complets contient aussi le nombre de fois où chaque article est cité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation des dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/juliettemontanteme/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# à faire si besoin\n",
    "nltk.download('wordnet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/home/arnaud/Documents/Mines1a/Informatique/Projet_informatique/data/CORD-19-research-challenge'\n",
    "metadata_path = f'{root_path}/metadata.csv'\n",
    "meta_df = pd.read_csv(metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_article(path):\n",
    "    file=open(root_path+\"/\"+path,'r')\n",
    "    article=json.load(file)\n",
    "    file.close()\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_text(article):\n",
    "    text_list = []\n",
    "    for entry in article['body_text']:\n",
    "        text_list.append(entry['text'])\n",
    "        text_list.append(\"\\n\")\n",
    "    \n",
    "    text_full=''.join(text_list)\n",
    "    return text_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date(x):\n",
    "    try:\n",
    "        return int(x[:4])\n",
    "    except TypeError:\n",
    "        return np.NaN\n",
    "\n",
    "def critère_date(date):\n",
    "    try:\n",
    "        return 1/(2020-date+1)\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df[\"date\"]=meta_df[\"publish_time\"].apply(lambda x: date(x))\n",
    "meta_df[\"note_date\"]=meta_df[\"date\"].apply(lambda x:critère_date(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation titre et abastract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()\n",
    "stop=stopwords.words('english')\n",
    "\n",
    "def preprocessing(x): #tokenisation,stop-words,lemmatization ...\n",
    "    clean = re.sub(r'['+string.punctuation + '’—”'+']', \"\", x.lower())\n",
    "    clean_text= re.sub(r'\\W+', ' ', clean)\n",
    "    a=\"\"\n",
    "    for i in clean_text.split():\n",
    "        if i not in stop :\n",
    "            a+=str(lemmatizer.lemmatize(i))+' '\n",
    "    return a\n",
    "\n",
    "def preprocessing_title(x): #tokenisation,stop-words,lemmatization ...\n",
    "    clean = re.sub(r'['+string.punctuation + '’—”'+']', \"\", x.lower())\n",
    "    clean_text= re.sub(r'\\W+', ' ', clean)\n",
    "    a=\"\"\n",
    "    for i in clean_text.split():\n",
    "        a+=str(lemmatizer.lemmatize(i))+' '\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df[\"title_process\"]=meta_df[\"title\"].apply(lambda x: preprocessing_title(str(x)))\n",
    "meta_df[\"abstract_process\"]=meta_df[\"abstract\"].apply(lambda x: preprocessing(str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nombre de citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_full_text(x):\n",
    "    if x[\"pdf_json_files\"]==True and x[\"pmc_json_files\"]==True:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_bool=meta_df[[\"pdf_json_files\",\"pmc_json_files\"]].isnull()\n",
    "meta_bool[\"has_full\"]=meta_bool.apply(lambda x:has_full_text(x),axis=1)\n",
    "meta_df_full_text=meta_df.loc[meta_bool[\"has_full\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_without_text=meta_df.loc[meta_bool[\"has_full\"]==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(path):\n",
    "    a=\"\"\n",
    "    for i in range (len(path)):\n",
    "        if path[i]==\";\":\n",
    "            break\n",
    "        a+=path[i]\n",
    "    return a\n",
    "\n",
    "def add_ref(article):\n",
    "    ref=article[\"bib_entries\"]\n",
    "\n",
    "    for i in ref.keys():\n",
    "        title=preprocessing_title(ref[i][\"title\"])\n",
    "        if title in set_ref:\n",
    "            dico_ref[title]+=1\n",
    "        else:\n",
    "            dico_ref[title]=1\n",
    "            set_ref.add(title)\n",
    "            \n",
    "def count_ref(x):\n",
    "    try:\n",
    "        path=x[\"pdf_json_files\"]\n",
    "        article=open_article(get_path(str(path)))\n",
    "       \n",
    "    except FileNotFoundError:\n",
    "        path=x[\"pmc_json_files\"]\n",
    "        article=open_article(get_path(str(path)))\n",
    "    add_ref(article)\n",
    "    return \"done\"\n",
    "\n",
    "def link_ref(x):\n",
    "    title=x\n",
    "    if title in set_ref:\n",
    "        return dico_ref[title]\n",
    "    return 0\n",
    "\n",
    "def path(x):\n",
    "    try:\n",
    "        try :\n",
    "            path=x[\"pdf_json_files\"]\n",
    "            article=open_article(get_path(str(path)))\n",
    "            return get_path(str(path))\n",
    "       \n",
    "        except FileNotFoundError:\n",
    "            path=x[\"pmc_json_files\"]\n",
    "            article=open_article(get_path(str(path)))\n",
    "            return get_path(str(path))\n",
    "    except FileNotFoundError:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico_ref=dict()\n",
    "set_ref=set()\n",
    "meta_df_full_text[\"ref\"]=meta_df_full_text[[\"pdf_json_files\",\"pmc_json_files\"]].apply(lambda x: count_ref(x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df[\"nb_ref_linked\"]=meta_df[\"title_process\"].apply(lambda x: link_ref(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ref=meta_df[\"nb_ref_linked\"].max()\n",
    "meta_df[\"note_ref\"]=meta_df[\"nb_ref_linked\"].apply(lambda x:x/max_ref)\n",
    "meta_df[\"path\"]=meta_df[[\"pdf_json_files\",\"pmc_json_files\"]].apply(lambda x:path(x),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_save=meta_df[[\"title_process\",\"abstract_process\",\"date\",\"path\",\"note_date\",\"nb_ref_linked\",\"note_ref\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df_save.to_csv(\"metadata_processed.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
